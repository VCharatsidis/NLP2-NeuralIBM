{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1\n",
    "\n",
    "NLP2 \n",
    "Author: Joost Bastings https://bastings.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'], ['36', 'e', 'Législature', ',', '2', 'ième', 'Session'])\n",
      "(['edited', 'HANSARD', '*', 'NUMBER', '1'], ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1'])\n",
      "(['contents'], ['table', 'DES', 'MATIÈRES'])\n",
      "(['Tuesday', ',', 'October', '12', ',', '1999'], ['le', 'mardi', '12', 'octobre', '1999'])\n"
     ]
    }
   ],
   "source": [
    "# check utils.py if you want to see how smart_reader and bitext_reader work in detail\n",
    "from utils import smart_reader, bitext_reader\n",
    "\n",
    "    \n",
    "def bitext_reader_demo(src_path, trg_path):\n",
    "  \"\"\"Demo of the bitext reader.\"\"\"\n",
    " \n",
    "  # create a reader\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "  # to see that it really works, try this:\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))  \n",
    "\n",
    "\n",
    "bitext_reader_demo(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 178928 sentences with max_length = 30\n"
     ]
    }
   ],
   "source": [
    "# To see how many sentences are left if you filter by length, you can do this:\n",
    "\n",
    "def demo_number_filtered_sentence_pairs(src_path, trg_path):\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  max_length = 30\n",
    "  bitext = bitext_reader(src_reader, trg_reader, max_length=max_length)   \n",
    "  num_sentences = sum([1 for _ in bitext])\n",
    "  print(\"There are {} sentences with max_length = {}\".format(num_sentences, max_length))\n",
    "  \n",
    "  \n",
    "demo_number_filtered_sentence_pairs(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create a vocabulary!\n",
    "\n",
    "We first define a class `Vocabulary` that helps us convert tokens (words) into numbers. This is useful later, because then we can e.g. index a word embedding table using the ID of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vocabulary.py to see how the Vocabulary class is defined\n",
    "from vocabulary import OrderedCounter, Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our Vocabulary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 36640\n",
      "Trimmed vocabulary size: 1005\n",
      "The index of \"<PAD>\" is: 0\n",
      "The index of \"<UNK>\" is: 1\n",
      "The index of \"the\" is: 5\n",
      "The token with index 0 is: <PAD>\n",
      "The token with index 1 is: <UNK>\n",
      "The token with index 2 is: <S>\n",
      "The token with index 3 is: </S>\n",
      "The token with index 4 is: <NULL>\n",
      "The token with index 5 is: the\n",
      "The token with index 6 is: .\n",
      "The token with index 7 is: ,\n",
      "The token with index 8 is: of\n",
      "The token with index 9 is: to\n",
      "The index of \"!@!_not_in_vocab_!@!\" is: 1\n"
     ]
    }
   ],
   "source": [
    "def vocabulary_demo():\n",
    "\n",
    "  # We used up a few lines in the previous example, so we set up\n",
    "  # our data generator again.\n",
    "  corpus = smart_reader(train_e_path)    \n",
    "\n",
    "  # Let's create a vocabulary given our (tokenized) corpus\n",
    "  vocabulary = Vocabulary(corpus=corpus)\n",
    "  print(\"Original vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we only keep the highest-frequency words\n",
    "  vocabulary_size=1000\n",
    "  vocabulary.trim(vocabulary_size)\n",
    "  print(\"Trimmed vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we can get word indexes using v.get_word_id():\n",
    "  for t in [\"<PAD>\", \"<UNK>\", \"the\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "\n",
    "  # And the inverse too, using v.i2t:\n",
    "  for i in range(10):\n",
    "    print(\"The token with index {} is: {}\".format(i, vocabulary.get_token(i)))\n",
    "\n",
    "  # Now let's try to get a word ID for a word not in the vocabulary\n",
    "  # we should get 1 (so, <UNK>)\n",
    "  for t in [\"!@!_not_in_vocab_!@!\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "    \n",
    "    \n",
    "vocabulary_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the vocabularies that we use further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 1005\n",
      "French vocabulary size: 1005\n",
      "\n",
      "A few English words:\n",
      "term\n",
      "social\n",
      "Ms.\n",
      "could\n",
      "population\n",
      "\n",
      "A few French words:\n",
      "pris\n",
      "agir\n",
      "humaines\n",
      "année\n",
      "vais\n"
     ]
    }
   ],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "print(\"English vocabulary size: {}\".format(len(vocabulary_e)))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))\n",
    "print(\"French vocabulary size: {}\".format(len(vocabulary_f)))\n",
    "print()\n",
    "\n",
    "\n",
    "def sample_words(vocabulary, n=5):\n",
    "  \"\"\"Print a few words from the vocabulary.\"\"\"\n",
    "  for _ in range(n):\n",
    "    token_id = np.random.randint(0, len(vocabulary) - 1)\n",
    "    print(vocabulary.get_token(token_id))\n",
    "\n",
    "\n",
    "print(\"A few English words:\")\n",
    "sample_words(vocabulary_e, n=5)\n",
    "print()\n",
    "\n",
    "print(\"A few French words:\")\n",
    "sample_words(vocabulary_f, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mini-batching\n",
    "\n",
    "With our vocabulary, we still need a method that converts a whole sentence to a sequence of IDs.\n",
    "And, to speed up training, we would like to get a so-called mini-batch at a time: multiple of such sequences together. So our function takes a corpus iterator and a vocabulary, and returns a mini-batch of shape [Batch, Time], where the first dimension indexes the sentences in the batch, and the second the time steps in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'],\n",
      "  ['36', 'e', 'Législature', ',', '2', 'ième', 'Session']),\n",
      " (['edited', 'HANSARD', '*', 'NUMBER', '1'],\n",
      "  ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1']),\n",
      " (['contents'], ['table', 'DES', 'MATIÈRES']),\n",
      " (['Tuesday', ',', 'October', '12', ',', '1999'],\n",
      "  ['le', 'mardi', '12', 'octobre', '1999'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 745 325   7 262   1   1]\n",
      " [  4   1   1  67   1 238   0   0]\n",
      " [  4   1   0   0   0   0   0   0]\n",
      " [  4   1   7 813 882   7 297   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1   1   1   7 254   1   1]\n",
      " [  1   1  62   1 250   0   0]\n",
      " [  1 463   1   0   0   0   0]\n",
      " [  6   1   1 840 295   0   0]]\n",
      "\n",
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['opening',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   'SECOND',\n",
      "   'SESSION',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   '36',\n",
      "   'TH',\n",
      "   'PARLIAMENT'],\n",
      "  ['ouverture',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   'DEUXIÈME',\n",
      "   'SESSION',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   '36E',\n",
      "   'LÉGISLATURE']),\n",
      " (['oaths', 'OF', 'OFFICE'], ['les', 'SERMENTS', 'De', 'OFFICE']),\n",
      " (['bill', 'C', '-', '1', '.'], ['projet', 'de', 'loi', 'C', '-', '1', '.']),\n",
      " (['introduction', 'and', 'first', 'reading'],\n",
      "  ['présentation', 'et', 'première', 'lecture'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 488 800   1   1 488 800   1   1   1]\n",
      " [  4   1 488   1   0   0   0   0   0   0   0]\n",
      " [  4  63  99  20 238   6   0   0   0   0   0]\n",
      " [  4 931  10 125 385   0   0   0   0   0   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1 239 542   1   1 239 542   1   1]\n",
      " [  9   1   1   1   0   0   0   0   0]\n",
      " [ 40   5  34  94  14 250   8   0   0]\n",
      " [832  13 227 336   0   0   0   0   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_reader = smart_reader(train_e_path)\n",
    "trg_reader = smart_reader(train_f_path)\n",
    "bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "\n",
    "for batch_id, batch in enumerate(iterate_minibatches(bitext, batch_size=4)):\n",
    "\n",
    "  print(\"This is the batch of data that we will train on, as tokens:\")\n",
    "  pprint(batch)\n",
    "  print()\n",
    "\n",
    "  x, y = prepare_data(batch, vocabulary_e, vocabulary_f)\n",
    "\n",
    "  print(\"These are our inputs (i.e. words replaced by IDs):\")\n",
    "  print(x)\n",
    "  print()\n",
    "  \n",
    "  print(\"These are the outputs (the foreign sentences):\")\n",
    "  print(y)\n",
    "  print()\n",
    "\n",
    "  if batch_id > 0:\n",
    "    break  # stop after the first batch, this is just a demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice the following:\n",
    "\n",
    "1. Every English sequence starts with a 4, the ID for < NULL \\>.\n",
    "2. The longest sequence in the batch contains no padding symbols. Any sequences shorter, however, will have padding zeros.\n",
    "\n",
    "With our input pipeline in place, now let's create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check neuralibm1.py for the Model code\n",
    "# Implement Neural IBM 1 on this class\n",
    "from neuralibm1 import NeuralIBM1Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Now that we have a model, we need to train it. To do so we define a Trainer class that takes our model as an argument and trains it, keeping track of some important information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check neuralibm1trainer.py for the Trainer code\n",
    "from neuralibm1trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, ?)\n",
      "\n",
      "(?, 1005)\n",
      "Training with B=128 max_length=30 lr=0.001 lr_decay=0.0\n",
      "WARNING:tensorflow:From C:\\Users\\chara\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 56.509724 accuracy 0.18 lr 0.001000\n",
      "Iter   200 loss 55.254192 accuracy 0.19 lr 0.001000\n",
      "Iter   300 loss 58.975578 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 46.941811 accuracy 0.19 lr 0.001000\n",
      "Iter   500 loss 47.825294 accuracy 0.22 lr 0.001000\n",
      "Iter   600 loss 59.471710 accuracy 0.18 lr 0.001000\n",
      "Iter   700 loss 46.716705 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 51.901016 accuracy 0.21 lr 0.001000\n",
      "Iter   900 loss 55.604546 accuracy 0.19 lr 0.001000\n",
      "Iter  1000 loss 48.352608 accuracy 0.20 lr 0.001000\n",
      "Iter  1100 loss 54.150208 accuracy 0.20 lr 0.001000\n",
      "Iter  1200 loss 47.189003 accuracy 0.21 lr 0.001000\n",
      "Iter  1300 loss 44.917984 accuracy 0.19 lr 0.001000\n",
      "Epoch 1 loss 51.822174 accuracy 0.19 val_aer 0.45 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 45.777847 accuracy 0.20 lr 0.001000\n",
      "Iter   200 loss 49.167969 accuracy 0.21 lr 0.001000\n",
      "Iter   300 loss 43.531013 accuracy 0.21 lr 0.001000\n",
      "Iter   400 loss 41.926762 accuracy 0.22 lr 0.001000\n",
      "Iter   500 loss 46.214104 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 41.396111 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss 47.753910 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 43.473644 accuracy 0.21 lr 0.001000\n",
      "Iter   900 loss 50.701569 accuracy 0.19 lr 0.001000\n",
      "Iter  1000 loss 47.242363 accuracy 0.20 lr 0.001000\n",
      "Iter  1100 loss 41.333149 accuracy 0.19 lr 0.001000\n",
      "Iter  1200 loss 45.889511 accuracy 0.20 lr 0.001000\n",
      "Iter  1300 loss 43.966583 accuracy 0.20 lr 0.001000\n",
      "Epoch 2 loss 45.216373 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 43.601757 accuracy 0.19 lr 0.001000\n",
      "Iter   200 loss 51.307495 accuracy 0.18 lr 0.001000\n",
      "Iter   300 loss 43.889404 accuracy 0.20 lr 0.001000\n",
      "Iter   400 loss 41.607948 accuracy 0.18 lr 0.001000\n",
      "Iter   500 loss 42.649132 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 40.709064 accuracy 0.19 lr 0.001000\n",
      "Iter   700 loss 47.245693 accuracy 0.19 lr 0.001000\n",
      "Iter   800 loss 46.014931 accuracy 0.19 lr 0.001000\n",
      "Iter   900 loss 42.875481 accuracy 0.20 lr 0.001000\n",
      "Iter  1000 loss 48.345188 accuracy 0.19 lr 0.001000\n",
      "Iter  1100 loss 45.307587 accuracy 0.19 lr 0.001000\n",
      "Iter  1200 loss 46.930317 accuracy 0.18 lr 0.001000\n",
      "Iter  1300 loss 44.212914 accuracy 0.18 lr 0.001000\n",
      "Epoch 3 loss 44.774050 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 44.551018 accuracy 0.21 lr 0.001000\n",
      "Iter   200 loss 46.685299 accuracy 0.18 lr 0.001000\n",
      "Iter   300 loss 49.878181 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 43.006989 accuracy 0.21 lr 0.001000\n",
      "Iter   500 loss 43.443382 accuracy 0.19 lr 0.001000\n",
      "Iter   600 loss 44.591320 accuracy 0.19 lr 0.001000\n",
      "Iter   700 loss 47.791260 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 48.478718 accuracy 0.19 lr 0.001000\n",
      "Iter   900 loss 45.509464 accuracy 0.18 lr 0.001000\n",
      "Iter  1000 loss 45.208176 accuracy 0.20 lr 0.001000\n",
      "Iter  1100 loss 43.192963 accuracy 0.19 lr 0.001000\n",
      "Iter  1200 loss 46.917961 accuracy 0.17 lr 0.001000\n",
      "Iter  1300 loss 44.210960 accuracy 0.21 lr 0.001000\n",
      "Epoch 4 loss 44.622904 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 43.338131 accuracy 0.20 lr 0.001000\n",
      "Iter   200 loss 41.029083 accuracy 0.21 lr 0.001000\n",
      "Iter   300 loss 49.194893 accuracy 0.20 lr 0.001000\n",
      "Iter   400 loss 40.303631 accuracy 0.18 lr 0.001000\n",
      "Iter   500 loss 49.168945 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss 38.484978 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss 43.162369 accuracy 0.19 lr 0.001000\n",
      "Iter   800 loss 43.109547 accuracy 0.20 lr 0.001000\n",
      "Iter   900 loss 39.696712 accuracy 0.21 lr 0.001000\n",
      "Iter  1000 loss 42.500725 accuracy 0.19 lr 0.001000\n",
      "Iter  1100 loss 50.408592 accuracy 0.20 lr 0.001000\n",
      "Iter  1200 loss 42.143856 accuracy 0.21 lr 0.001000\n",
      "Iter  1300 loss 46.486942 accuracy 0.18 lr 0.001000\n",
      "Epoch 5 loss 44.528722 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 42.800869 accuracy 0.21 lr 0.001000\n",
      "Iter   200 loss 43.834408 accuracy 0.19 lr 0.001000\n",
      "Iter   300 loss 43.037327 accuracy 0.22 lr 0.001000\n",
      "Iter   400 loss 44.578671 accuracy 0.19 lr 0.001000\n",
      "Iter   500 loss 35.007057 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 43.549126 accuracy 0.19 lr 0.001000\n",
      "Iter   700 loss 38.870987 accuracy 0.21 lr 0.001000\n",
      "Iter   800 loss 38.701763 accuracy 0.20 lr 0.001000\n",
      "Iter   900 loss 49.343597 accuracy 0.19 lr 0.001000\n",
      "Iter  1000 loss 41.922260 accuracy 0.21 lr 0.001000\n",
      "Iter  1100 loss 45.298409 accuracy 0.21 lr 0.001000\n",
      "Iter  1200 loss 45.799809 accuracy 0.20 lr 0.001000\n",
      "Iter  1300 loss 40.686100 accuracy 0.23 lr 0.001000\n",
      "Epoch 6 loss 44.467637 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 40.624966 accuracy 0.20 lr 0.001000\n",
      "Iter   200 loss 44.280693 accuracy 0.20 lr 0.001000\n",
      "Iter   300 loss 44.385735 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 45.746635 accuracy 0.19 lr 0.001000\n",
      "Iter   500 loss 45.147717 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 43.850506 accuracy 0.19 lr 0.001000\n",
      "Iter   700 loss 49.860153 accuracy 0.21 lr 0.001000\n",
      "Iter   800 loss 43.301559 accuracy 0.21 lr 0.001000\n",
      "Iter   900 loss 45.170517 accuracy 0.18 lr 0.001000\n",
      "Iter  1000 loss 39.387657 accuracy 0.21 lr 0.001000\n",
      "Iter  1100 loss 38.146370 accuracy 0.20 lr 0.001000\n",
      "Iter  1200 loss 44.357262 accuracy 0.16 lr 0.001000\n",
      "Iter  1300 loss 47.027126 accuracy 0.20 lr 0.001000\n",
      "Epoch 7 loss 44.421837 accuracy 0.20 val_aer 0.43 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 46.962513 accuracy 0.19 lr 0.001000\n",
      "Iter   200 loss 47.855484 accuracy 0.18 lr 0.001000\n",
      "Iter   300 loss 45.610970 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 41.540848 accuracy 0.20 lr 0.001000\n",
      "Iter   500 loss 43.623203 accuracy 0.19 lr 0.001000\n",
      "Iter   600 loss 47.865372 accuracy 0.20 lr 0.001000\n",
      "Iter   700 loss 45.564804 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 42.959503 accuracy 0.19 lr 0.001000\n",
      "Iter   900 loss 43.911037 accuracy 0.18 lr 0.001000\n",
      "Iter  1000 loss 38.754585 accuracy 0.21 lr 0.001000\n",
      "Iter  1100 loss 42.162144 accuracy 0.21 lr 0.001000\n",
      "Iter  1200 loss 44.674759 accuracy 0.20 lr 0.001000\n",
      "Iter  1300 loss 43.457790 accuracy 0.20 lr 0.001000\n",
      "Epoch 8 loss 44.381350 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 44.502544 accuracy 0.22 lr 0.001000\n",
      "Iter   200 loss 42.886169 accuracy 0.20 lr 0.001000\n",
      "Iter   300 loss 47.609596 accuracy 0.20 lr 0.001000\n",
      "Iter   400 loss 43.982765 accuracy 0.20 lr 0.001000\n",
      "Iter   500 loss 43.448669 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss 50.934875 accuracy 0.21 lr 0.001000\n",
      "Iter   700 loss 44.639771 accuracy 0.21 lr 0.001000\n",
      "Iter   800 loss 44.653107 accuracy 0.20 lr 0.001000\n",
      "Iter   900 loss 47.676041 accuracy 0.19 lr 0.001000\n",
      "Iter  1000 loss 41.122196 accuracy 0.21 lr 0.001000\n",
      "Iter  1100 loss 41.304794 accuracy 0.20 lr 0.001000\n",
      "Iter  1200 loss 44.953232 accuracy 0.21 lr 0.001000\n",
      "Iter  1300 loss 47.505367 accuracy 0.20 lr 0.001000\n",
      "Epoch 9 loss 44.359143 accuracy 0.20 val_aer 0.44 val_acc 0.19\n",
      "Model saved in file: ./model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 40.897095 accuracy 0.19 lr 0.001000\n",
      "Iter   200 loss 42.896278 accuracy 0.22 lr 0.001000\n",
      "Iter   300 loss 40.566959 accuracy 0.21 lr 0.001000\n",
      "Iter   400 loss 38.799683 accuracy 0.20 lr 0.001000\n",
      "Iter   500 loss 47.325455 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss 39.828186 accuracy 0.20 lr 0.001000\n",
      "Iter   700 loss 45.846142 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 46.991886 accuracy 0.20 lr 0.001000\n",
      "Iter   900 loss 42.210510 accuracy 0.20 lr 0.001000\n",
      "Iter  1000 loss 47.752136 accuracy 0.19 lr 0.001000\n",
      "Iter  1100 loss 41.485912 accuracy 0.21 lr 0.001000\n",
      "Iter  1200 loss 43.733406 accuracy 0.19 lr 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  1300 loss 46.624062 accuracy 0.20 lr 0.001000\n",
      "Epoch 10 loss 44.332971 accuracy 0.20 val_aer 0.44 val_acc 0.18\n",
      "Model saved in file: ./model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=128  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "  \n",
    "  # our model\n",
    "  model = NeuralIBM1Model(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess)\n",
    "  \n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer(\n",
    "    model, train_e_path, train_f_path, \n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=10, batch_size=batch_size, \n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
